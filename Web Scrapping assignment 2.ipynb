{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374b52b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Find the search form and input values\u001b[39;00m\n\u001b[0;32m     18\u001b[0m form \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_search\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 19\u001b[0m \u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqryTxt\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_title\n\u001b[0;32m     20\u001b[0m form\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcboWorkin\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m location\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Submit the form\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Step 2: Enter search criteria and click the search button\n",
    "    job_title = \"Data Analyst\"\n",
    "    location = \"Bangalore\"\n",
    "\n",
    "    # Find the search form and input values\n",
    "    form = soup.find('form', {'name': 'job_search'})\n",
    "    form.find('input', {'name': 'qryTxt'}).attrs['value'] = job_title\n",
    "    form.find('input', {'name': 'cboWorkin'}).attrs['value'] = location\n",
    "\n",
    "    # Submit the form\n",
    "    response = requests.post(url, data=form.form_data)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Step 4: Scrape data for the first 10 jobs\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        job_listings = soup.find_all('div', class_='search_listing')\n",
    "\n",
    "        data_list = []\n",
    "        for job in job_listings[:10]:\n",
    "            job_title = job.find('span', class_='job_title').text.strip()\n",
    "            job_location = job.find('span', class_='job_loc').text.strip()\n",
    "            company_name = job.find('span', class_='comp_name').text.strip()\n",
    "            experience_required = job.find('li', {'title': 'Experience'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Job Title': job_title,\n",
    "                'Job Location': job_location,\n",
    "                'Company Name': company_name,\n",
    "                'Experience Required': experience_required\n",
    "            })\n",
    "\n",
    "        # Step 5: Create a DataFrame\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch search results. Status Code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629feeb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Find the search form and input values\u001b[39;00m\n\u001b[0;32m     18\u001b[0m form \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_search\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 19\u001b[0m \u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqryTxt\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_title\n\u001b[0;32m     20\u001b[0m form\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcboWorkin\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m location\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Submit the form\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Step 2: Enter search criteria and click the search button\n",
    "    job_title = \"Data Scientist\"\n",
    "    location = \"Bangalore\"\n",
    "\n",
    "    # Find the search form and input values\n",
    "    form = soup.find('form', {'name': 'job_search'})\n",
    "    form.find('input', {'name': 'qryTxt'}).attrs['value'] = job_title\n",
    "    form.find('input', {'name': 'cboWorkin'}).attrs['value'] = location\n",
    "\n",
    "    # Submit the form\n",
    "    response = requests.post(url, data=form.form_data)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Step 4: Scrape data for the first 10 jobs\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        job_listings = soup.find_all('div', class_='search_listing')\n",
    "\n",
    "        data_list = []\n",
    "        for job in job_listings[:10]:\n",
    "            job_title = job.find('span', class_='job_title').text.strip()\n",
    "            job_location = job.find('span', class_='job_loc').text.strip()\n",
    "            company_name = job.find('span', class_='comp_name').text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Job Title': job_title,\n",
    "                'Job Location': job_location,\n",
    "                'Company Name': company_name\n",
    "            })\n",
    "\n",
    "        # Step 5: Create a DataFrame\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch search results. Status Code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e97476",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Find the search form and input values\u001b[39;00m\n\u001b[0;32m     18\u001b[0m form \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_search\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 19\u001b[0m \u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqryTxt\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_title\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Submit the form to get initial search results\u001b[39;00m\n\u001b[0;32m     22\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, data\u001b[38;5;241m=\u001b[39mform\u001b[38;5;241m.\u001b[39mform_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Step 2: Enter search criteria and click the search button\n",
    "    job_title = \"Data Scientist\"\n",
    "    location = \"Delhi/NCR\"\n",
    "\n",
    "    # Find the search form and input values\n",
    "    form = soup.find('form', {'name': 'job_search'})\n",
    "    form.find('input', {'name': 'qryTxt'}).attrs['value'] = job_title\n",
    "\n",
    "    # Submit the form to get initial search results\n",
    "    response = requests.post(url, data=form.form_data)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Step 4: Apply location and salary filters\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find location filter checkbox and select Delhi/NCR\n",
    "        location_checkbox = soup.find('input', {'name': 'chkDelhi'})\n",
    "        if location_checkbox:\n",
    "            location_checkbox.attrs['checked'] = 'checked'\n",
    "\n",
    "        # Find salary filter checkbox and select 3-6 lakhs\n",
    "        salary_checkbox = soup.find('input', {'name': 'chk3-6'})\n",
    "        if salary_checkbox:\n",
    "            salary_checkbox.attrs['checked'] = 'checked'\n",
    "\n",
    "        # Submit the form to get updated search results with filters\n",
    "        response = requests.post(url, data=soup.form.form_data)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Step 5: Scrape data for the first 10 jobs\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            job_listings = soup.find_all('div', class_='search_listing')\n",
    "\n",
    "            data_list = []\n",
    "            for job in job_listings[:10]:\n",
    "                job_title = job.find('span', class_='job_title').text.strip()\n",
    "                job_location = job.find('span', class_='job_loc').text.strip()\n",
    "                company_name = job.find('span', class_='comp_name').text.strip()\n",
    "                experience_required = job.find('li', {'title': 'Experience'}).text.strip()\n",
    "\n",
    "                data_list.append({\n",
    "                    'Job Title': job_title,\n",
    "                    'Job Location': job_location,\n",
    "                    'Company Name': company_name,\n",
    "                    'Experience Required': experience_required\n",
    "                })\n",
    "\n",
    "            # Step 6: Create a DataFrame\n",
    "            df = pd.DataFrame(data_list)\n",
    "\n",
    "            # Display the DataFrame\n",
    "            print(df)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch filtered search results. Status Code: {response.status_code}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch initial search results. Status Code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4137249",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_list) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[0;32m     41\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     sunglasses_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_sunglasses_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sunglasses_data:\n\u001b[0;32m     45\u001b[0m         data_list\u001b[38;5;241m.\u001b[39mextend(sunglasses_data)\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mscrape_sunglasses_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sunglasses \u001b[38;5;129;01min\u001b[39;00m sunglasses_list:\n\u001b[1;32m---> 15\u001b[0m     brand \u001b[38;5;241m=\u001b[39m \u001b[43msunglasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_2WkVRV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     product_description \u001b[38;5;241m=\u001b[39m sunglasses\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     price \u001b[38;5;241m=\u001b[39m sunglasses\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_30jeq3\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape sunglasses data from a given Flipkart page\n",
    "def scrape_sunglasses_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        sunglasses_list = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        \n",
    "        data_list = []\n",
    "        for sunglasses in sunglasses_list:\n",
    "            brand = sunglasses.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "            product_description = sunglasses.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "            price = sunglasses.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Brand': brand,\n",
    "                'Product Description': product_description,\n",
    "                'Price': price\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for the first 100 sunglasses listings on Flipkart\n",
    "def main():\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = f\"{base_url}/search?q=sunglasses\"\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    # Scrape data from multiple pages until we have data for 100 sunglasses\n",
    "    page_number = 1\n",
    "    while len(data_list) < 100:\n",
    "        url = f\"{search_url}&page={page_number}\"\n",
    "        sunglasses_data = scrape_sunglasses_data(url)\n",
    "\n",
    "        if sunglasses_data:\n",
    "            data_list.extend(sunglasses_data)\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0128ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating       Review Summary  \\\n",
      "0       5            Just wow!   \n",
      "1       5    Worth every penny   \n",
      "2       5  Best in the market!   \n",
      "3       5            Wonderful   \n",
      "4       5       Classy product   \n",
      "..    ...                  ...   \n",
      "95      5   Highly recommended   \n",
      "96      5            Wonderful   \n",
      "97      4            Very Good   \n",
      "98      5            Brilliant   \n",
      "99      5       Classy product   \n",
      "\n",
      "                                          Full Review  \n",
      "0                          Perfect Product!!READ MORE  \n",
      "1   Feeling awesome after getting the delivery of ...  \n",
      "2                                Good CameraREAD MORE  \n",
      "3                     This is amazing at allREAD MORE  \n",
      "4   Camera is awesomeBest battery backupA performe...  \n",
      "..                                                ...  \n",
      "95  Thanks Flipkart For this amazing deal! I had a...  \n",
      "96  Excellent Fabulous Adorable Iphone 11 Value fo...  \n",
      "97  I switched to IOS for long term use and for be...  \n",
      "98  Perfect iPhone on this budget!! Camera and the...  \n",
      "99        Outstanding performance this phoneREAD MORE  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape reviews data from the given Flipkart page\n",
    "def scrape_reviews_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        reviews_list = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "        data_list = []\n",
    "        for review in reviews_list:\n",
    "            rating = review.find('div', {'class': '_3LWZlK'}).text.strip()\n",
    "            review_summary = review.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "            full_review = review.find('div', {'class': 't-ZTKy'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Rating': rating,\n",
    "                'Review Summary': review_summary,\n",
    "                'Full Review': full_review\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for the first 100 reviews on Flipkart\n",
    "def main():\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    product_url = (\"/apple-iphone-11-black-64-gb/product-reviews/\"\n",
    "                   \"itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    # Scrape data from multiple pages until we have data for 100 reviews\n",
    "    page_number = 1\n",
    "    while len(data_list) < 100:\n",
    "        url = f\"{base_url}{product_url}&page={page_number}\"\n",
    "        reviews_data = scrape_reviews_data(url)\n",
    "\n",
    "        if reviews_data:\n",
    "            data_list.extend(reviews_data)\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7389918",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_list) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[0;32m     41\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     sneakers_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_sneakers_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sneakers_data:\n\u001b[0;32m     45\u001b[0m         data_list\u001b[38;5;241m.\u001b[39mextend(sneakers_data)\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mscrape_sneakers_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sneaker \u001b[38;5;129;01min\u001b[39;00m sneakers_list:\n\u001b[1;32m---> 15\u001b[0m     brand \u001b[38;5;241m=\u001b[39m \u001b[43msneaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_2WkVRV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     product_description \u001b[38;5;241m=\u001b[39m sneaker\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     price \u001b[38;5;241m=\u001b[39m sneaker\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_30jeq3\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape sneakers data from a given Flipkart page\n",
    "def scrape_sneakers_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        sneakers_list = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        \n",
    "        data_list = []\n",
    "        for sneaker in sneakers_list:\n",
    "            brand = sneaker.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "            product_description = sneaker.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "            price = sneaker.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Brand': brand,\n",
    "                'Product Description': product_description,\n",
    "                'Price': price\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for the first 100 sneakers on Flipkart\n",
    "def main():\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = f\"{base_url}/search?q=sneakers\"\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    # Scrape data from multiple pages until we have data for 100 sneakers\n",
    "    page_number = 1\n",
    "    while len(data_list) < 100:\n",
    "        url = f\"{search_url}&page={page_number}\"\n",
    "        sneakers_data = scrape_sneakers_data(url)\n",
    "\n",
    "        if sneakers_data:\n",
    "            data_list.extend(sneakers_data)\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1077db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page. Status Code: 503\n",
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape laptops data from a given Amazon page\n",
    "def scrape_laptops_data(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        laptops_list = soup.find_all('div', {'data-asin': True})\n",
    "\n",
    "        data_list = []\n",
    "        for laptop in laptops_list[:10]:\n",
    "            title = laptop.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "            ratings = laptop.find('span', {'class': 'a-icon-alt'})\n",
    "            ratings = ratings.text if ratings else \"Not Available\"\n",
    "            price = laptop.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Title': title,\n",
    "                'Ratings': ratings,\n",
    "                'Price': price\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for the first 10 laptops on Amazon\n",
    "def main():\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k=Laptop\"\n",
    "\n",
    "    # Scrape data from the search results page\n",
    "    laptops_data = scrape_laptops_data(search_url)\n",
    "\n",
    "    if laptops_data:\n",
    "        # Display the scraped data\n",
    "        for index, laptop in enumerate(laptops_data, start=1):\n",
    "            print(f\"\\nLaptop {index}:\")\n",
    "            print(f\"Title: {laptop['Title']}\")\n",
    "            print(f\"Ratings: {laptop['Ratings']}\")\n",
    "            print(f\"Price: {laptop['Price']}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f4ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page. Status Code: 404\n",
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape top quotes data from azquotes.com\n",
    "def scrape_top_quotes_data(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        quotes_list = soup.find_all('div', {'class': 'wrap-block'})\n",
    "\n",
    "        data_list = []\n",
    "        for quote_block in quotes_list[:1000]:\n",
    "            quote = quote_block.find('a', {'class': 'title'}).text.strip()\n",
    "            author = quote_block.find('a', {'class': 'author'}).text.strip()\n",
    "            quote_type = quote_block.find('div', {'class': 'greyText smallText left'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Quote': quote,\n",
    "                'Author': author,\n",
    "                'Type Of Quotes': quote_type\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for the Top 1000 Quotes of All Time\n",
    "def main():\n",
    "    base_url = \"https://www.azquotes.com\"\n",
    "    top_quotes_url = f\"{base_url}/top-quotes\"\n",
    "\n",
    "    # Scrape data from the Top Quotes page\n",
    "    top_quotes_data = scrape_top_quotes_data(top_quotes_url)\n",
    "\n",
    "    if top_quotes_data:\n",
    "        # Display the scraped data\n",
    "        for index, quote_data in enumerate(top_quotes_data, start=1):\n",
    "            print(f\"\\nQuote {index}:\")\n",
    "            print(f\"Quote: {quote_data['Quote']}\")\n",
    "            print(f\"Author: {quote_data['Author']}\")\n",
    "            print(f\"Type Of Quotes: {quote_data['Type Of Quotes']}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffa35be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m gk_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/general-knowledge/list-of-all-prime-ministers-of-india-1460473340-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Scrape data from the List of all Prime Ministers page\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m prime_ministers_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_prime_ministers_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgk_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prime_ministers_data:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(prime_ministers_data)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mscrape_prime_ministers_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     14\u001b[0m prime_ministers_table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtableizer-table\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     16\u001b[0m data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprime_ministers_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     18\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for former Prime Ministers from jagranjosh.com\n",
    "def scrape_prime_ministers_data(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        prime_ministers_table = soup.find('table', {'class': 'tableizer-table'})\n",
    "\n",
    "        data_list = []\n",
    "        for row in prime_ministers_table.find_all('tr')[1:]:\n",
    "            columns = row.find_all('td')\n",
    "            name = columns[0].text.strip()\n",
    "            born_dead = columns[1].text.strip()\n",
    "            term_of_office = columns[2].text.strip()\n",
    "            remarks = columns[3].text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Name': name,\n",
    "                'Born-Dead': born_dead,\n",
    "                'Term of Office': term_of_office,\n",
    "                'Remarks': remarks\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for former Prime Ministers of India\n",
    "def main():\n",
    "    base_url = \"https://www.jagranjosh.com\"\n",
    "    gk_url = f\"{base_url}/general-knowledge/list-of-all-prime-ministers-of-india-1460473340-1\"\n",
    "\n",
    "    # Scrape data from the List of all Prime Ministers page\n",
    "    prime_ministers_data = scrape_prime_ministers_data(gk_url)\n",
    "\n",
    "    if prime_ministers_data:\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(prime_ministers_data)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aab4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for the 50 most expensive cars from motor1.com\n",
    "def scrape_expensive_cars_data(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        cars_list = soup.find_all('div', {'class': 'list-item'})\n",
    "\n",
    "        data_list = []\n",
    "        for car in cars_list:\n",
    "            car_name = car.find('div', {'class': 'title'}).text.strip()\n",
    "            car_price = car.find('div', {'class': 'price'}).text.strip()\n",
    "\n",
    "            data_list.append({\n",
    "                'Car Name': car_name,\n",
    "                'Price': car_price\n",
    "            })\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape data for the 50 most expensive cars\n",
    "def main():\n",
    "    base_url = \"https://www.motor1.com\"\n",
    "    search_url = f\"{base_url}/search/?q=50+most+expensive+cars\"\n",
    "\n",
    "    # Scrape data from the search results page\n",
    "    search_results_data = scrape_expensive_cars_data(search_url)\n",
    "\n",
    "    if search_results_data:\n",
    "        # Display the scraped data\n",
    "        for index, car_data in enumerate(search_results_data, start=1):\n",
    "            print(f\"\\nCar {index}:\")\n",
    "            print(f\"Car Name: {car_data['Car Name']}\")\n",
    "            print(f\"Price: {car_data['Price']}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f24096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
