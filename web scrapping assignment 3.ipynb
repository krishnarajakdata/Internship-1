{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = \"https://www.amazon.in/s?k=\"\n",
    "    search_query = urllib.parse.quote_plus(product)\n",
    "    url = base_url + search_query\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all(\"div\", {\"class\": \"s-result-item\"})\n",
    "\n",
    "    for product in products:\n",
    "        title_element = product.find(\"span\", {\"class\": \"a-text-normal\"})\n",
    "        price_element = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "        if title_element and price_element:\n",
    "            title = title_element.text.strip()\n",
    "            price = price_element.text.strip()\n",
    "            print(\"Product:\", title)\n",
    "            print(\"Price:\", price)\n",
    "            print(\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon: \")\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba5e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa755c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "def scrape_product_details(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    brand_name = soup.find(\"a\", {\"id\": \"bylineInfo\"})\n",
    "    if brand_name:\n",
    "        brand_name = brand_name.text.strip()\n",
    "    else:\n",
    "        brand_name = \"-\"\n",
    "\n",
    "    product_name = soup.find(\"span\", {\"id\": \"productTitle\"})\n",
    "    if product_name:\n",
    "        product_name = product_name.text.strip()\n",
    "    else:\n",
    "        product_name = \"-\"\n",
    "\n",
    "    price = soup.find(\"span\", {\"id\": \"priceblock_ourprice\"})\n",
    "    if price:\n",
    "        price = price.text.strip()\n",
    "    else:\n",
    "        price = \"-\"\n",
    "\n",
    "    return_exchange = soup.find(\"div\", {\"id\": \"RETURNS_POLICY\"})\n",
    "    if return_exchange:\n",
    "        return_exchange = return_exchange.text.strip()\n",
    "    else:\n",
    "        return_exchange = \"-\"\n",
    "\n",
    "    expected_delivery = soup.find(\"div\", {\"id\": \"ddmDeliveryMessage\"})\n",
    "    if expected_delivery:\n",
    "        expected_delivery = expected_delivery.text.strip()\n",
    "    else:\n",
    "        expected_delivery = \"-\"\n",
    "\n",
    "    availability = soup.find(\"div\", {\"id\": \"availability\"})\n",
    "    if availability:\n",
    "        availability = availability.text.strip()\n",
    "    else:\n",
    "        availability = \"-\"\n",
    "\n",
    "    return brand_name, product_name, price, return_exchange, expected_delivery, availability\n",
    "\n",
    "def scrape_amazon_products(product):\n",
    "    base_url = \"https://www.amazon.in/s?k=\"\n",
    "    search_query = urllib.parse.quote_plus(product)\n",
    "    url = base_url + search_query\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "    all_products_data = []\n",
    "\n",
    "    for page in range(1, 4):  # Scraping the first 3 pages\n",
    "        url_with_page = f\"{url}&page={page}\"\n",
    "        response = requests.get(url_with_page, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all(\"div\", {\"class\": \"s-result-item\"})\n",
    "\n",
    "        for product in products:\n",
    "            product_link = product.find(\"a\", {\"class\": \"a-link-normal\"})\n",
    "            if product_link:\n",
    "                product_url = \"https://www.amazon.in\" + product_link['href']\n",
    "                details = scrape_product_details(product_url)\n",
    "                all_products_data.append(details)\n",
    "\n",
    "    return all_products_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon: \")\n",
    "    product_data = scrape_amazon_products(user_input)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(product_data, columns=[\"Brand Name\", \"Product Name\", \"Price\", \"Return/Exchange\", \n",
    "                                              \"Expected Delivery\", \"Availability\"])\n",
    "    df[\"Product URL\"] = [f\"https://www.amazon.in/s?k={urllib.parse.quote_plus(user_input)}\" for _ in range(len(df))]\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"amazon_products.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0069b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape images from Google Images\n",
    "def scrape_images(keyword, num_images):\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open Google Images\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "    # Find the search bar and enter the keyword\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    # Find the search button and click it\n",
    "    search_button = driver.find_element_by_xpath(\"//button[@type='submit']\")\n",
    "    search_button.click()\n",
    "\n",
    "    # Scroll to load more images\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the page source using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all image elements\n",
    "    img_elements = soup.find_all('img', class_='rg_i')\n",
    "\n",
    "    # Create a directory to save the images\n",
    "    os.makedirs(keyword, exist_ok=True)\n",
    "\n",
    "    # Download the images\n",
    "    for i, img_element in enumerate(img_elements[:num_images]):\n",
    "        img_url = img_element['src']\n",
    "        img_name = os.path.join(keyword, f\"{keyword}_{i+1}.jpg\")\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(img_name, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "            print(f\"Downloaded {keyword} image {i+1}/{num_images}\")\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Keywords and number of images to scrape for each keyword\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images = 10\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    scrape_images(keyword, num_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    base_url = \"https://www.flipkart.com/search?q=\"\n",
    "    url = base_url + search_query\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    smartphones = soup.find_all(\"div\", {\"class\": \"_1AtVbE\"})  # class may vary, please inspect the page source\n",
    "\n",
    "    all_smartphones_data = []\n",
    "\n",
    "    for smartphone in smartphones:\n",
    "        brand_name = smartphone.find(\"div\", {\"class\": \"_4rR01T\"}).text.strip()\n",
    "        smartphone_name = smartphone.find(\"a\", {\"class\": \"IRpwTa\"}).text.strip()\n",
    "        colour = smartphone.find(\"div\", {\"class\": \"_2WkVRV\"}).text.strip()\n",
    "        features = smartphone.find_all(\"li\", {\"class\": \"rgWa7D\"})\n",
    "\n",
    "        ram, storage, primary_camera, secondary_camera, display_size, battery_capacity, price, product_url = \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"\n",
    "\n",
    "        for feature in features:\n",
    "            feature_text = feature.text.strip()\n",
    "            if \"RAM\" in feature_text:\n",
    "                ram = feature_text\n",
    "            elif \"ROM\" in feature_text:\n",
    "                storage = feature_text\n",
    "            elif \"Primary Camera\" in feature_text:\n",
    "                primary_camera = feature_text\n",
    "            elif \"Secondary Camera\" in feature_text:\n",
    "                secondary_camera = feature_text\n",
    "            elif \"Display Size\" in feature_text:\n",
    "                display_size = feature_text\n",
    "            elif \"Battery Capacity\" in feature_text:\n",
    "                battery_capacity = feature_text\n",
    "\n",
    "        try:\n",
    "            price = smartphone.find(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"}).text.strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            product_url = \"https://www.flipkart.com\" + smartphone.find(\"a\", {\"class\": \"IRpwTa\"})['href']\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        smartphone_data = [brand_name, smartphone_name, colour, ram, storage, primary_camera,\n",
    "                           secondary_camera, display_size, battery_capacity, price, product_url]\n",
    "\n",
    "        all_smartphones_data.append(smartphone_data)\n",
    "\n",
    "    return all_smartphones_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    smartphones_data = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(smartphones_data, columns=[\"Brand Name\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage(ROM)\",\n",
    "                                                 \"Primary Camera\", \"Secondary Camera\", \"Display Size\",\n",
    "                                                 \"Battery Capacity\", \"Price\", \"Product URL\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"flipkart_smartphones.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googlemaps\n",
    "import googlemaps\n",
    "\n",
    "def get_coordinates(city):\n",
    "    # Replace 'YOUR_API_KEY' with your actual Google Maps API key\n",
    "    gmaps = googlemaps.Client(key='YOUR_API_KEY')\n",
    "\n",
    "    # Geocoding an address\n",
    "    geocode_result = gmaps.geocode(city)\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    if geocode_result:\n",
    "        location = geocode_result[0]['geometry']['location']\n",
    "        latitude = location['lat']\n",
    "        longitude = location['lng']\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city name: \")\n",
    "    coordinates = get_coordinates(city)\n",
    "    if coordinates:\n",
    "        print(f\"Latitude: {coordinates[0]}, Longitude: {coordinates[1]}\")\n",
    "    else:\n",
    "        print(\"Could not find coordinates for the specified city.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "    # Send HTTP GET request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "\n",
    "        laptop_details = []\n",
    "\n",
    "        for laptop in laptops:\n",
    "            details = {}\n",
    "            details['Name'] = laptop.find('div', class_='heading-6').text.strip()\n",
    "            details['Price'] = laptop.find('div', class_='smprice').text.strip()\n",
    "            details['Specifications'] = laptop.find('div', class_='Specs-Wrap').text.strip()\n",
    "            details['URL'] = 'https://www.digit.in' + laptop.find('a')['href']\n",
    "            laptop_details.append(details)\n",
    "\n",
    "        return laptop_details\n",
    "    else:\n",
    "        print('Failed to fetch the webpage.')\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "    if gaming_laptops:\n",
    "        df = pd.DataFrame(gaming_laptops)\n",
    "        df.to_csv('gaming_laptops_digit.csv', index=False)\n",
    "        print('Scraping complete. Data saved to \"gaming_laptops_digit.csv\"')\n",
    "    else:\n",
    "        print('Scraping failed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470190b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    # Send HTTP GET request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        billionaires = soup.find_all('div', class_='personName')\n",
    "\n",
    "        billionaire_details = []\n",
    "\n",
    "        for billionaire in billionaires:\n",
    "            details = {}\n",
    "            details['Name'] = billionaire.find('div', class_='personName').text.strip()\n",
    "            details['Rank'] = billionaire.find('div', class_='rank').text.strip()\n",
    "            details['Net worth'] = billionaire.find('div', class_='netWorth').text.strip()\n",
    "            details['Age'] = billionaire.find('div', class_='age').text.strip()\n",
    "            details['Citizenship'] = billionaire.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            details['Source'] = billionaire.find('div', class_='source').text.strip()\n",
    "            details['Industry'] = billionaire.find('div', class_='category').text.strip()\n",
    "            billionaire_details.append(details)\n",
    "\n",
    "        return billionaire_details\n",
    "    else:\n",
    "        print('Failed to fetch the webpage.')\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    billionaires_data = scrape_forbes_billionaires()\n",
    "\n",
    "    if billionaires_data:\n",
    "        df = pd.DataFrame(billionaires_data)\n",
    "        df.to_csv('forbes_billionaires.csv', index=False)\n",
    "        print('Scraping complete. Data saved to \"forbes_billionaires.csv\"')\n",
    "    else:\n",
    "        print('Scraping failed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "\n",
    "# Define your API key\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define the YouTube video ID you want to extract comments from\n",
    "VIDEO_ID = \"VIDEO_ID\"\n",
    "\n",
    "def get_video_comments(api_key, video_id, max_results=500):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Request the video resource\n",
    "    video_response = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=max_results,\n",
    "        order='relevance',\n",
    "        textFormat='plainText'\n",
    "    ).execute()\n",
    "\n",
    "    comments = []\n",
    "\n",
    "    # Iterate over each comment\n",
    "    for item in video_response['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        comment_text = comment['textDisplay']\n",
    "        comment_likes = comment['likeCount']\n",
    "        comment_time = datetime.strptime(comment['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "        comments.append({\n",
    "            'Comment': comment_text,\n",
    "            'Likes': comment_likes,\n",
    "            'Time': comment_time\n",
    "        })\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comments_data = get_video_comments(API_KEY, VIDEO_ID)\n",
    "\n",
    "    print(\"Extracted Comments:\")\n",
    "    for comment in comments_data:\n",
    "        print(f\"Comment: {comment['Comment']}\")\n",
    "        print(f\"Likes: {comment['Likes']}\")\n",
    "        print(f\"Time: {comment['Time']}\")\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de36eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/search?search_keywords=London%2C%20England&country=England&city=London&date_from=2024-02-01&date_to=2024-02-04&number_of_guests=1\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        hostels = soup.find_all('div', class_='property-card')\n",
    "\n",
    "        hostel_details = []\n",
    "\n",
    "        for hostel in hostels:\n",
    "            details = {}\n",
    "            details['Name'] = hostel.find('h2', class_='title').text.strip()\n",
    "            details['Distance from City Centre'] = hostel.find('span', class_='description').text.strip()\n",
    "            details['Ratings'] = hostel.find('div', class_='score orange big').text.strip()\n",
    "            details['Total Reviews'] = hostel.find('div', class_='reviews').text.strip()\n",
    "            details['Overall Reviews'] = hostel.find('div', class_='keyword').text.strip()\n",
    "            details['Privates From Price'] = hostel.find('div', class_='price-col').text.strip().split('\\n')[0].strip()\n",
    "            details['Dorms From Price'] = hostel.find('div', class_='price-col').text.strip().split('\\n')[1].strip()\n",
    "            details['Facilities'] = [facility.text.strip() for facility in hostel.find_all('div', class_='facilities')]\n",
    "            details['Property Description'] = hostel.find('div', class_='ratings').next_sibling.strip()\n",
    "            \n",
    "            hostel_details.append(details)\n",
    "\n",
    "        return hostel_details\n",
    "    else:\n",
    "        print('Failed to fetch the webpage.')\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hostels_data = scrape_hostels_in_london()\n",
    "\n",
    "    if hostels_data:\n",
    "        for index, hostel in enumerate(hostels_data, start=1):\n",
    "            print(f\"Hostel {index}:\")\n",
    "            print(f\"Name: {hostel['Name']}\")\n",
    "            print(f\"Distance from City Centre: {hostel['Distance from City Centre']}\")\n",
    "            print(f\"Ratings: {hostel['Ratings']}\")\n",
    "            print(f\"Total Reviews: {hostel['Total Reviews']}\")\n",
    "            print(f\"Overall Reviews: {hostel['Overall Reviews']}\")\n",
    "            print(f\"Privates From Price: {hostel['Privates From Price']}\")\n",
    "            print(f\"Dorms From Price: {hostel['Dorms From Price']}\")\n",
    "            print(f\"Facilities: {', '.join(hostel['Facilities'])}\")\n",
    "            print(f\"Property Description: {hostel['Property Description']}\")\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        print('Scraping failed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae9f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
