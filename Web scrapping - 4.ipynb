{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a364aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Launch the web browser\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Launch the web browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Wikipedia page\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "\n",
    "# Find the table containing the most viewed videos\n",
    "table = driver.find_element(By.CSS_SELECTOR, \".wikitable\")\n",
    "\n",
    "# Find all rows in the table\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Loop through each row to extract data\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    \n",
    "    # Extract data from each column\n",
    "    rank = columns[0].text\n",
    "    name = columns[1].text\n",
    "    artist = columns[2].text\n",
    "    upload_date = columns[3].text\n",
    "    views = columns[4].text\n",
    "    \n",
    "    # Append data to lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(rank_list)):\n",
    "    print(\"Rank:\", rank_list[i])\n",
    "    print(\"Name:\", name_list[i])\n",
    "    print(\"Artist:\", artist_list[i])\n",
    "    print(\"Upload Date:\", upload_date_list[i])\n",
    "    print(\"Views:\", views_list[i])\n",
    "    print()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89ff686",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.bcci.tvhttps', port=443): Max retries exceeded with url: //www.bcci.tv/fixtures?platform=international&type=men (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020579BDE1A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000020579BDE1A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.bcci.tvhttps', port=443): Max retries exceeded with url: //www.bcci.tv/fixtures?platform=international&type=men (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020579BDE1A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m fixtures_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.bcci.tv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfixtures_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Send a GET request to the international fixtures page\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m fixtures_response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixtures_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fixtures_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Parse the HTML content of the international fixtures page\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.bcci.tvhttps', port=443): Max retries exceeded with url: //www.bcci.tv/fixtures?platform=international&type=men (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020579BDE1A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the BCCI website\n",
    "response = requests.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the link to the international fixtures page\n",
    "    fixtures_link = soup.find(\"a\", text=\"Fixtures\")[\"href\"]\n",
    "    \n",
    "    # Construct the URL for the international fixtures page\n",
    "    fixtures_url = f\"https://www.bcci.tv{fixtures_link}\"\n",
    "    \n",
    "    # Send a GET request to the international fixtures page\n",
    "    fixtures_response = requests.get(fixtures_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if fixtures_response.status_code == 200:\n",
    "        # Parse the HTML content of the international fixtures page\n",
    "        fixtures_soup = BeautifulSoup(fixtures_response.content, \"html.parser\")\n",
    "        \n",
    "        # Find all the fixtures\n",
    "        fixtures = fixtures_soup.find_all(\"div\", class_=\"js-list\")\n",
    "        \n",
    "        # Iterate over each fixture to extract details\n",
    "        for fixture in fixtures:\n",
    "            series = fixture.find(\"strong\", class_=\"fixture__name\").text.strip()\n",
    "            place = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "            date = fixture.find(\"span\", class_=\"fixture__date\").text.strip()\n",
    "            time = fixture.find(\"span\", class_=\"fixture__time\").text.strip()\n",
    "            \n",
    "            # Print the details of the fixture\n",
    "            print(\"Series:\", series)\n",
    "            print(\"Place:\", place)\n",
    "            print(\"Date:\", date)\n",
    "            print(\"Time:\", time)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to retrieve international fixtures page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve BCCI website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160daf55",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m homepage_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(homepage_response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the Economy page\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m economy_link \u001b[38;5;241m=\u001b[39m \u001b[43mhomepage_soup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEconomy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Construct the URL for the Economy page\u001b[39;00m\n\u001b[0;32m     17\u001b[0m economy_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhomepage_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00meconomy_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the homepage of statisticstimes.com\n",
    "homepage_url = \"http://statisticstimes.com/\"\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if homepage_response.status_code == 200:\n",
    "    # Parse the HTML content of the homepage\n",
    "    homepage_soup = BeautifulSoup(homepage_response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the link to the Economy page\n",
    "    economy_link = homepage_soup.find(\"a\", text=\"Economy\")[\"href\"]\n",
    "    \n",
    "    # Construct the URL for the Economy page\n",
    "    economy_url = f\"{homepage_url}{economy_link}\"\n",
    "    \n",
    "    # Send a GET request to the Economy page\n",
    "    economy_response = requests.get(economy_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if economy_response.status_code == 200:\n",
    "        # Parse the HTML content of the Economy page\n",
    "        economy_soup = BeautifulSoup(economy_response.content, \"html.parser\")\n",
    "        \n",
    "        # Find the State-wise GDP table\n",
    "        gdp_table = economy_soup.find(\"table\", id=\"table_id\")\n",
    "        \n",
    "        # Extract data from the table\n",
    "        rows = gdp_table.find_all(\"tr\")\n",
    "        \n",
    "        # Initialize lists to store data\n",
    "        data = []\n",
    "        \n",
    "        # Loop through each row to extract data\n",
    "        for row in rows[1:]:  # Skipping header row\n",
    "            columns = row.find_all(\"td\")\n",
    "            rank = columns[0].text.strip()\n",
    "            state = columns[1].text.strip()\n",
    "            gdp_18_19 = columns[2].text.strip()\n",
    "            gdp_19_20 = columns[3].text.strip()\n",
    "            share_18_19 = columns[4].text.strip()\n",
    "            gdp_billion = columns[5].text.strip()\n",
    "            \n",
    "            # Append data to the list\n",
    "            data.append({\n",
    "                \"Rank\": rank,\n",
    "                \"State\": state,\n",
    "                \"GSDP(18-19) - Current Prices\": gdp_18_19,\n",
    "                \"GSDP(19-20) - Current Prices\": gdp_19_20,\n",
    "                \"Share(18-19)\": share_18_19,\n",
    "                \"GDP($ billion)\": gdp_billion\n",
    "            })\n",
    "        \n",
    "        # Print the scraped data\n",
    "        for item in data:\n",
    "            print(\"Rank:\", item[\"Rank\"])\n",
    "            print(\"State:\", item[\"State\"])\n",
    "            print(\"GSDP(18-19) - Current Prices:\", item[\"GSDP(18-19) - Current Prices\"])\n",
    "            print(\"GSDP(19-20) - Current Prices:\", item[\"GSDP(19-20) - Current Prices\"])\n",
    "            print(\"Share(18-19):\", item[\"Share(18-19)\"])\n",
    "            print(\"GDP($ billion):\", item[\"GDP($ billion)\"])\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to retrieve the Economy page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the homepage of statisticstimes.com.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31b5eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Loop through each repository to extract data\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repositories:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Extract repository title\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3 lh-condensed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Extract repository description\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     description \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol-9 color-text-secondary my-1 pr-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the GitHub homepage\n",
    "homepage_url = \"https://github.com/\"\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if homepage_response.status_code == 200:\n",
    "    # Parse the HTML content of the homepage\n",
    "    homepage_soup = BeautifulSoup(homepage_response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the link to the Trending page\n",
    "    trending_link = homepage_soup.find(\"a\", href=\"/trending\")[\"href\"]\n",
    "    \n",
    "    # Construct the URL for the Trending page\n",
    "    trending_url = f\"{homepage_url}{trending_link}\"\n",
    "    \n",
    "    # Send a GET request to the Trending page\n",
    "    trending_response = requests.get(trending_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if trending_response.status_code == 200:\n",
    "        # Parse the HTML content of the Trending page\n",
    "        trending_soup = BeautifulSoup(trending_response.content, \"html.parser\")\n",
    "        \n",
    "        # Find all the repositories\n",
    "        repositories = trending_soup.find_all(\"article\", class_=\"Box-row\")\n",
    "        \n",
    "        # Loop through each repository to extract data\n",
    "        for repo in repositories:\n",
    "            # Extract repository title\n",
    "            title = repo.find(\"h1\", class_=\"h3 lh-condensed\").text.strip()\n",
    "            \n",
    "            # Extract repository description\n",
    "            description = repo.find(\"p\", class_=\"col-9 color-text-secondary my-1 pr-4\").text.strip()\n",
    "            \n",
    "            # Extract contributors count\n",
    "            contributors_count_tag = repo.find(\"a\", href=True, attrs={\"data-hovercard-type\": \"repository\"})\n",
    "            contributors_count = contributors_count_tag.text.strip() if contributors_count_tag else \"Not available\"\n",
    "            \n",
    "            # Extract language used\n",
    "            language = repo.find(\"span\", class_=\"d-inline-block ml-0 mr-3\").text.strip()\n",
    "            \n",
    "            # Print the details of the repository\n",
    "            print(\"Repository title:\", title)\n",
    "            print(\"Repository description:\", description)\n",
    "            print(\"Contributors count:\", contributors_count)\n",
    "            print(\"Language used:\", language)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to retrieve the Trending page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the GitHub homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93655148",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m homepage_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(homepage_response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the Charts page\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m charts_link \u001b[38;5;241m=\u001b[39m \u001b[43mhomepage_soup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCharts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Construct the URL for the Charts page\u001b[39;00m\n\u001b[0;32m     17\u001b[0m charts_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhomepage_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcharts_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Billboard homepage\n",
    "homepage_url = \"https://www.billboard.com/\"\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if homepage_response.status_code == 200:\n",
    "    # Parse the HTML content of the homepage\n",
    "    homepage_soup = BeautifulSoup(homepage_response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the link to the Charts page\n",
    "    charts_link = homepage_soup.find(\"a\", text=\"Charts\")[\"href\"]\n",
    "    \n",
    "    # Construct the URL for the Charts page\n",
    "    charts_url = f\"{homepage_url}{charts_link}\"\n",
    "    \n",
    "    # Send a GET request to the Charts page\n",
    "    charts_response = requests.get(charts_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if charts_response.status_code == 200:\n",
    "        # Parse the HTML content of the Charts page\n",
    "        charts_soup = BeautifulSoup(charts_response.content, \"html.parser\")\n",
    "        \n",
    "        # Find the link to the Hot 100 page\n",
    "        hot_100_link = charts_soup.find(\"a\", text=\"Hot 100\")[\"href\"]\n",
    "        \n",
    "        # Construct the URL for the Hot 100 page\n",
    "        hot_100_url = f\"{homepage_url}{hot_100_link}\"\n",
    "        \n",
    "        # Send a GET request to the Hot 100 page\n",
    "        hot_100_response = requests.get(hot_100_url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if hot_100_response.status_code == 200:\n",
    "            # Parse the HTML content of the Hot 100 page\n",
    "            hot_100_soup = BeautifulSoup(hot_100_response.content, \"html.parser\")\n",
    "            \n",
    "            # Find the top 100 songs\n",
    "            songs = hot_100_soup.find_all(\"li\", class_=\"chart-list__element\")\n",
    "            \n",
    "            # Loop through each song to extract data\n",
    "            for song in songs:\n",
    "                # Extract song name\n",
    "                song_name = song.find(\"span\", class_=\"chart-element__information__song\").text.strip()\n",
    "                \n",
    "                # Extract artist name\n",
    "                artist_name = song.find(\"span\", class_=\"chart-element__information__artist\").text.strip()\n",
    "                \n",
    "                # Extract last week rank\n",
    "                last_week_rank = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--last\").text.strip()\n",
    "                \n",
    "                # Extract peak rank\n",
    "                peak_rank = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--peak\").text.strip()\n",
    "                \n",
    "                # Extract weeks on board\n",
    "                weeks_on_board = song.find(\"span\", class_=\"chart-element__meta text--center color--secondary text--week\").text.strip()\n",
    "                \n",
    "                # Print the details of the song\n",
    "                print(\"Song name:\", song_name)\n",
    "                print(\"Artist name:\", artist_name)\n",
    "                print(\"Last week rank:\", last_week_rank)\n",
    "                print(\"Peak rank:\", peak_rank)\n",
    "                print(\"Weeks on board:\", weeks_on_board)\n",
    "                print()\n",
    "        else:\n",
    "            print(\"Failed to retrieve the Hot 100 page.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve the Charts page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the Billboard homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2614a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Extracting the details\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m book_name \u001b[38;5;241m=\u001b[39m \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m author_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m volumes_sold \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the provided URL\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the table containing the details of the highest-selling novels\n",
    "    table = soup.find(\"table\", class_=\"in-article sortable\")\n",
    "    \n",
    "    # Extract data from the table\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "    \n",
    "    # Loop through each row to extract data\n",
    "    for row in rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        \n",
    "        # Extracting the details\n",
    "        book_name = columns[1].text.strip()\n",
    "        author_name = columns[2].text.strip()\n",
    "        volumes_sold = columns[3].text.strip()\n",
    "        publisher = columns[4].text.strip()\n",
    "        genre = columns[5].text.strip()\n",
    "        \n",
    "        # Print the details of the novel\n",
    "        print(\"Book name:\", book_name)\n",
    "        print(\"Author name:\", author_name)\n",
    "        print(\"Volumes sold:\", volumes_sold)\n",
    "        print(\"Publisher:\", publisher)\n",
    "        print(\"Genre:\", genre)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4398c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the IMDb URL\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all the TV series listed on the page\n",
    "    series_list = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "    \n",
    "    # Loop through each TV series to extract data\n",
    "    for series in series_list:\n",
    "        # Extracting the details\n",
    "        name = series.find(\"a\").text.strip()\n",
    "        year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\")\n",
    "        rating = series.find(\"span\", class_=\"ipl-rating-star__rating\").text.strip()\n",
    "        votes = series.find(\"span\", attrs={\"name\": \"nv\"}).text.strip()\n",
    "        \n",
    "        # Print the details of the TV series\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Year span:\", year_span)\n",
    "        print(\"Rating:\", rating)\n",
    "        print(\"Votes:\", votes)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e91a0bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the \"Show All Dataset\" page\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m all_dataset_link \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mView ALL Data Sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Construct the URL for the \"Show All Dataset\" page\u001b[39;00m\n\u001b[0;32m     17\u001b[0m all_dataset_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhomepage_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mall_dataset_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the UCI Machine Learning Repository homepage\n",
    "homepage_url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(homepage_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the homepage\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the link to the \"Show All Dataset\" page\n",
    "    all_dataset_link = soup.find(\"a\", text=\"View ALL Data Sets\")[\"href\"]\n",
    "    \n",
    "    # Construct the URL for the \"Show All Dataset\" page\n",
    "    all_dataset_url = f\"{homepage_url}{all_dataset_link}\"\n",
    "    \n",
    "    # Send a GET request to the \"Show All Dataset\" page\n",
    "    all_dataset_response = requests.get(all_dataset_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if all_dataset_response.status_code == 200:\n",
    "        # Parse the HTML content of the \"Show All Dataset\" page\n",
    "        all_dataset_soup = BeautifulSoup(all_dataset_response.content, \"html.parser\")\n",
    "        \n",
    "        # Find the list of datasets\n",
    "        datasets = all_dataset_soup.find_all(\"table\")[5].find_all(\"tr\")[1:]\n",
    "        \n",
    "        # Loop through each dataset to extract data\n",
    "        for dataset in datasets:\n",
    "            # Extracting the details\n",
    "            columns = dataset.find_all(\"td\")\n",
    "            dataset_name = columns[0].text.strip()\n",
    "            data_type = columns[1].text.strip()\n",
    "            task = columns[2].text.strip()\n",
    "            attribute_type = columns[3].text.strip()\n",
    "            num_instances = columns[4].text.strip()\n",
    "            num_attributes = columns[5].text.strip()\n",
    "            year = columns[6].text.strip()\n",
    "            \n",
    "            # Print the details of the dataset\n",
    "            print(\"Dataset name:\", dataset_name)\n",
    "            print(\"Data type:\", data_type)\n",
    "            print(\"Task:\", task)\n",
    "            print(\"Attribute type:\", attribute_type)\n",
    "            print(\"No of instances:\", num_instances)\n",
    "            print(\"No of attributes:\", num_attributes)\n",
    "            print(\"Year:\", year)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to retrieve the 'Show All Dataset' page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the UCI Machine Learning Repository homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0388bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
